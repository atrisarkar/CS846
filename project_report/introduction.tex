% !TEX encoding = UTF-8 Unicode
% !TEX root = project.tex

\section{Introduction}
\label{sec:intro}

Over the last decade we have observed a significant shift in the way software is developed. This is driven mainly by big open source organizations like the Apache and the Mozilla Foundation with generous support from organizations like Google. Most of the popular software systems like the Android Project,projects in Hadoop and other big data ecosystem, dynamic search engine frameworks like lucene, elasticsearch are few examples of how open source development has reshaped the way software is developed. Even organizations like IBM who had traditionally followed a proprietary model recognize the changing landscapes and have started to have strong presence in the open source model like IBM's sponsorship of the Eclipse foundation[7]. As a researcher, it becomes imperative to evaluate our understanding of development processes in light of this changing landscape.
Developers in a typical open source system can fall under two broad non-exclusive categories. In the first category we have the set of developers who are assumed to have a detailed and in depth knowledge on the design and implementation of the project. They are the developers who have spent significant time with the project, have higher rate of contribution and risen the ranks over time to be in a position where they can act as gatekeepers to any change committed in the project. In the second category we have rest of the contributors and developers who are not yet in the core group, however contribute code to the project's  repository by adding new feature implementation or resolving any reported defects. One of the roles which the first category or the core group of developers exercise is the role of a reviewer. This role expects the them to exercise the following activities:\\
1. Review any changes made to the code base by contributing developers to check if the contributions meet the accepted project standards. This role is similar to that of a reviewer in a code-review process.\\
2. To analyse impact of the contributions made by developers in the project in general. This includes but is not limited to checking if the contributing changes are accompanied by required test-cases, if the contribution runs the risk of breaking existing functionality or even analysing what all needs to be modified to accomplish the intended change. Tasks in this role is usually categorised under Change Impact Analysis (CIA).[2] \\
In this project, we concern ourselves with this second activity. Based on github data, we have seen that the number of commits in open source projects during peak development activity can go upto a thousand a day, whereas the number of core developers in a project usually are under hundred[3]. Under such a scenario, Our objective is to assist the reviewer in analysing the impact of a certain change and help him in deciding whether to accept that change or not.\\

\section{Analysing Change Impact}

Changes made to a software often has the potential of causing collateral side effect and/or a ripple effect. This ripple effect is measured in terms of the likelihood that a change to a particular set of files may cause problems in the rest of the software [2]. The first step in Change Impact Analysis (CIA) is to look into the list of files that are changed as a part of the change set. Using this list various CIA techniques are used to estimate a list of files that are probably affected by that change. This list of affected files are called Estimated Impact Set (EIS). There are several techniques proposed to generate this EIS and Li et.al [2] provides an exhaustive survey of such techniques.\\
One such technique is based on the hypothesis that impact of a file on another can be measured by looking at how frequently the two files are changed together. The following real world example explains the idea further.\\
Most software systems today support internationalization (i18n) by which text in user interfaces can be changed easily from one language to another. To implement this, developers are forbidden to hardcode UI texts in english. Instead, the UI files contain a key literal in place of the text which is replaced by the actual language text during runtime. Thus, developers in such systems always need to check-in the language file (which contains the key-value mapping for each language) whenever they add any new field in the UI. In such scenario we would observe the UI file and the language file to have been checked in together in majority of cases. If a new change set comes in for review to the reviewer where he observes only the UI file to have been modified, there is a high chance that the developer would have broken the internationalization functionality of the software.\\
Our tool generates the list list of impacted files (EIS) by mining the historical commit data for the files in the change sets. We use top-k association rule mining algorithm to find out the files which have been frequently changed together. Using the algorithm we generate the rules of the form $A \rightarrow B$ where $A$ is the files in the change set and $B$ is the EIS set which represents the files impacted due to changes in $A$. Each file in set $B$ is also mapped to a confidence value which gives an estimate of the likelihood of the file being impacted by changes in $A$. The following section gives a brief introduction to top-k association rule mining algorithm and the process of generating the rules.\\  

\subsection{Top-K Association Rule Mining}

Association Rule mining is the process of discovering associations within items in a set of transaction. A classic application of association rule mining is in supermarkets. Stores look at transaction data of customers and apply association rule mining to generate rules of the kind \{bread\}$\rightarrow$\{milk\} which means that if a customer buys bread, he is very likely to buy milk too.\\
In our context we can relate items to files and transaction to the change sets. Thus, the objective of association rule mining is formally stated as follows [4] :\\
Let \textbf{I}=$\{a_{1},a_{2},..a_{n}\}$ be a finite set of files in the system. Querying information from the repository data, we can get a set of check-ins \textbf{T}=$\{t_{1},t_{2}..t_{m}\}$ where each check-in $t_{j}\subseteq\textbf{I}$ $(1 \le j \le m)$ represents a single commit in the repository. An itemset is a set of files \textbf{X} $\subseteq$ \textbf{I}. The support of an itemset \textbf{X} is denoted as \textit{sup(\textbf{X})} and is defined as the number of check-ins that contain files in \textbf{X}. An association rule \textbf{X}$\rightarrow$\textbf{Y} is a relationship between two itemsets \textbf{X}, \textbf{Y} such that \textbf{X}, \textbf{Y} $\subseteq$ \textbf{I} and \textbf{X} $\cap$ \textbf{Y}= $\emptyset$. The support of a rule \textbf{X} $\rightarrow$ \textbf{Y} is defined as 

\begin{equation}
\textit{sup}(\textbf{X} \rightarrow \textbf{Y}) = \textit{sup}(\textbf{X}\cup\textbf{Y}) / |\textbf{T}|.
\end{equation}
 

The confidence of a rule \textbf{X} $\rightarrow$ \textbf{Y} is defined as 

\begin{equation}
conf(\textbf{X} \rightarrow \textbf{Y}) = \textit{sup}(\textbf{X}\cup\textbf{Y}) / \textit{sup}(\textbf{X}). 
\end{equation}

Our objective of mining association rules [5] is to find all association rules in a repository having a support no less than a defined threshold \textit{minsup} and a confidence no less
than a defined threshold \textit{minconf}. These generated association rules measures the impact of a file on another. \\

Figure 1 shows a snapshot of check-ins in a repository (left) and the association rules found for \textit{minsup} = 0.5 and \textit{minconf} = 0.5 (right). Mining associations is done in two steps [5]. Step 1 is to discover all frequent itemsets in the database (itemsets appearing in at least \textit{minsup} $\times$ |T| transactions). Step 2 is to generate association rules by using the frequent itemsets found in step 1. For each frequent itemset \textbf{X}, pairs of frequent itemsets \textbf{P} and \textbf{Q = X - P} are selected to form rules of the form P$\rightarrow$Q. For each such rule \textbf{P$\rightarrow$Q}, if \textit{sup(P$\rightarrow$Q) $\ge$ minsup} and \textit{conf(P$\rightarrow$Q) $\ge$ minconf}, the rule is selected. In our case, we impose an additional constraint that \textbf{P} should be the set of files in the change set proposed by the contributing developer.

\begin{table}[htbp]
  \centering
  \caption{}
  \subfloat[Change Sets]{%
    \hspace{.5cm}%
    \begin{tabular}{|c|c|}
        \hline
        \textbf{ID}       & \textbf{ChangeSets}    \\
        \hline
        $t_{1}$       & \{a,b,c,e,f,g\}   \\
        \hline
	$t_{2}$       & \{a,b,c,d,e,f\}   \\
        \hline
	$t_{3}$       & \{a,b,e,f\}   \\
        \hline
	$t_{4}$       & \{b,f,g\}   \\
        \hline
    \end{tabular}%
    \hspace{.5cm}%
  }\hspace{1cm}
  \subfloat[Association Rules]{%
    \hspace{.5cm}%
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{ID}       & \textbf{Rule}       & \textbf{Support} & \textbf{Confidence} \\
        
        \hline
        $r_{1}$        & \{a\} $\rightarrow$ \{b\}       & 0.75 & 1 \\
        \hline
        $r_{1}$        & \{a\} $\rightarrow$ \{c,e,f\}       & 0.5 & 0.6 \\
        \hline
        $r_{1}$        & \{a,b\} $\rightarrow$ \{e,f\}       & 0.75 & 1 \\
        \hline
        ..       & ...       & ... & .. \\
        \hline
    \end{tabular}%
    \hspace{.5cm}%
  }
\end{table}


Top-K association rule mining [4] is an efficient extension on the traditional A-priori algorithm by Agarwal and Srikanth [5] which mines the \textit{k} strongest association rules found in the itemsets.


\subsection{Pull Requests}

In this project we have built our tool based on the Github data and thus it is important to introduce the terminologies involved. Projects in the Github system works similar to any other open source projects where new, inexperienced or external developers need to get their changes reviewed by the project owners. In Github this happens through Pull Requests.\\

\textit{"Pull requests let you tell others about changes you've pushed to a repository on GitHub. Once a pull request is sent, interested parties can review the set of changes, discuss potential modifications, and even push follow-up commits if necessary."} [6] \\

Thus, Pull requests contain the set of files changed by the developer. The reviewer reviews the pull request and he has the option to either \textit{approve} or \textit{reject} it. Our RICE tool integrates with the Pull Request UI of Github to display an additional list of impacted files along with their confidence value to make this decision process easier for the reviewers.   



