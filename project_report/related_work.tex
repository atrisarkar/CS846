% !TEX encoding = UTF-8 Unicode
% !TEX root = project.tex

\section{Related Work}
\label{sec:related}

There has been some previous research which has proposed the idea of mining repositories to find simultaneously checked-in files to help developers in various ways. Zimmermann, Thomas, et al.~\cite{zimmermann2005mining} and Ying, Annie TT, et al.~\cite{ying2004predicting} have looked at exactly the same idea and proposed to use association rule mining to dig out related files from the repository checked-in data. However, both of their strategies were designed keeping Concurrent Version System (CVS) in mind as the source control system and they had to do a lot of preprocessing on the data to fit it in their algorithm. Over the last decade, there has been increasing focus on maintaining more metadata information in source control systems. For example, GitHub has an open API which can be utilised to gather commit information for any open source project in a structured way.

Hatari is similar to our work in that it shows files which may be problematic by relating their version history to a bug database and finding out whether there were risks in the past~\cite{sliwerski2005hatari}. Hatari is static and does not adapt to new change information~\cite{kim2007predicting}, whereas our work adapts as more and more files are added to the pull requests.

Walker, Robert J., et al.~\cite{walker2006lightweight} proposed an approach to assess and communicate technical risks, which is based on weakly-estimated knowledge, historical change behavior, and the current structure of the software. This approach is built on a probabilistic algorithm, which is combined with dependency analysis and history mining~\cite{lehnert2011review}. CVS repositories were used for their study. In our work, we make use of GitHub repositories and the algorithm we use is not probabilistic.

Kim, Sunghun, et al.~\cite{kim2007predicting} proposed an approach to predict the most fault-prone entities and files. They surmise that faults don't happen in isolation, but rather in bursts of several related files. In their approach, the authors cache locations that are likely to have faults using a fault prediction algorithm. So, a developer can consult the cache at the time when a fault is fixed to find out other fault-prone areas related to the fix. This is different from our approach in that we don't perform any caching or fault prediction as it is done in this work.

Nagappan, et al.~\cite{nagappan2005use} use code churn, which is a measure of the amount of code change taking place within a software unit over time, to predict the defect density in software systems. Kim, Sunghun, et al.~\cite{kim2008classifying} proposed a technique for predicting latent software bugs, called change classification, which uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes or clean changes, thereby predicting the existence of bugs in software changes.






